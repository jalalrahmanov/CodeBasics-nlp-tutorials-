{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52n-h9KGuj9F"
      },
      "source": [
        "<h2 align=\"center\">Spacy Tokenization Tutorial</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pWg4OCz8uj9J"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbImIqSGuj9L"
      },
      "source": [
        "Create blank language object and tokenize words in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "id": "4TVvq8yyuj9L",
        "outputId": "91a19b0d-a80d-4554-a766-a76f364090ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "Strange\n",
            "loves\n",
            "pav\n",
            "bhaji\n",
            "of\n",
            "mumbai\n",
            "as\n",
            "it\n",
            "costs\n",
            "only\n",
            "2\n",
            "$\n",
            "per\n",
            "plate\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# en = english\n",
        "# spacy language models yazib axtarsan goresisen butun desteklediyi dilleri\n",
        "# blank ile yazanda tek tek cumleye ya da soze bolmek olmur, yeni doc.sents ya sonra da\n",
        "# sentence.words yazmaq olmur - load edende amma packageni elemek olur.\n",
        "# bu blank deyesen sadece durgu isarelerine gore bolur\n",
        "# burda nlp adinda language object yaradirsan. muxtelif usullari var\n",
        "# bele sade yaratmaq blank ile. load ile package daxil ederek daha advancedini yaratmaq. bu pipelinedir. gelecekde baxasiyiq.\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ancaq bele durgu isareleri ile bolmek olur vessalam. az dili ucun package yoxdur\n",
        "nlp_az = spacy.blank(\"az\")\n",
        "\n",
        "doc_az = nlp_az(\"Bəxtiyar Vahabzadə 16 avqust 1925-ci ildə Şəki \\\n",
        "şəhərində fəhlə ailəsində anadan olmuşdur. Mahmud Vahabzadənin \\\n",
        "həyat yoldaşı isə 1905-ci il hadisələri zamanı yetim qalmış \\\n",
        "Vartaşendən olan erməni qızı olub, daha sonra Şəkidə Allahverdi \\\n",
        "adlı bir nəfərin himayəsində Gülzar adı ilə böyüdülüb. \")\n",
        "\n",
        "for token in doc_az:\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "z98ackJ0vwGL",
        "outputId": "079c5bcf-9c38-4fc0-e02d-afaf44568195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bəxtiyar\n",
            "Vahabzadə\n",
            "16\n",
            "avqust\n",
            "1925-ci\n",
            "ildə\n",
            "Şəki\n",
            "şəhərində\n",
            "fəhlə\n",
            "ailəsində\n",
            "anadan\n",
            "olmuşdur\n",
            ".\n",
            "Mahmud\n",
            "Vahabzadənin\n",
            "həyat\n",
            "yoldaşı\n",
            "isə\n",
            "1905-ci\n",
            "il\n",
            "hadisələri\n",
            "zamanı\n",
            "yetim\n",
            "qalmış\n",
            "Vartaşendən\n",
            "olan\n",
            "erməni\n",
            "qızı\n",
            "olub\n",
            ",\n",
            "daha\n",
            "sonra\n",
            "Şəkidə\n",
            "Allahverdi\n",
            "adlı\n",
            "bir\n",
            "nəfərin\n",
            "himayəsində\n",
            "Gülzar\n",
            "adı\n",
            "ilə\n",
            "böyüdülüb\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english dilini isledek\n",
        "nlp_2 = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# iki cumlelik metn daxil edirik\n",
        "doc_2 = nlp_2('Creating blank language object gives a tokenizer \\\n",
        "and an empty pipeline. We will look more into language pipelines \\\n",
        "in next tutorial')\n",
        "\n",
        "for sentence_token in doc_2.sents:\n",
        "    print(sentence_token)\n",
        "    print(type(sentence_token))"
      ],
      "metadata": {
        "id": "Z567FvbowzR1",
        "outputId": "0c101ee6-41c8-4bcc-ad9a-2954e2836b32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating blank language object gives a tokenizer and an empty pipeline.\n",
            "<class 'spacy.tokens.span.Span'>\n",
            "We will look more into language pipelines in next tutorial\n",
            "<class 'spacy.tokens.span.Span'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence_token in doc_2.sents:\n",
        "    for word_token in sentence_token:\n",
        "      print(word_token)\n",
        "      print(type(word_token))"
      ],
      "metadata": {
        "id": "OPXB6ifGy31o",
        "outputId": "9b9ef11f-35a6-4a49-c5e3-f725bf031b07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "blank\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "language\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "object\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "gives\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "a\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "tokenizer\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "and\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "an\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "empty\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "pipeline\n",
            "<class 'spacy.tokens.token.Token'>\n",
            ".\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "We\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "will\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "look\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "more\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "into\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "language\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "pipelines\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "in\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "next\n",
            "<class 'spacy.tokens.token.Token'>\n",
            "tutorial\n",
            "<class 'spacy.tokens.token.Token'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # burda documentationdan baxsan (spacy models for languages) azerbaycan dilinin packagesi yoxdu\n",
        "# # english ucun ise 4 package var meselen\n",
        "# nlp_2 = spacy.load(\"???\")\n",
        "\n",
        "# # iki cumlelik metn daxil edirik\n",
        "# doc_2 = nlp_2('Creating blank language object gives a tokenizer \\\n",
        "# and an empty pipeline. We will look more into language pipelines \\\n",
        "# in next tutorial')\n",
        "\n",
        "# for token in doc_2.sents:\n",
        "#     print(token)"
      ],
      "metadata": {
        "id": "MrZq_jkzxJOl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPjNY95Guj9N"
      },
      "source": [
        "Creating blank language object gives a tokenizer and an empty pipeline. We will look more into language pipelines in next tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzKrVfEeuj9O"
      },
      "source": [
        "<img src=\"https://github.com/jalalrahmanov/CodeBasics-nlp-tutorials-/blob/main/4_tokenization/spacy_blank_pipeline.jpg?raw=1\" height=100, width=500/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDgQi-DHuj9P"
      },
      "source": [
        "<h3>Using index to grab tokens</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "q7K_MFAFuj9Q",
        "outputId": "52841ab0-e001-4e7f-abf3-536156e2af4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dr."
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "doc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "scrolled": true,
        "id": "xsMipsipuj9R",
        "outputId": "319f6743-9a76-4bfc-c823-a0b985aa094f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Strange'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "token = doc[1]\n",
        "token.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "scrolled": false,
        "id": "WIzcEJjZuj9S",
        "outputId": "78c07802-a382-4d2f-a700-6a759d9e617d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " 'ancestors',\n",
              " 'check_flag',\n",
              " 'children',\n",
              " 'cluster',\n",
              " 'conjuncts',\n",
              " 'dep',\n",
              " 'dep_',\n",
              " 'doc',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ent_iob',\n",
              " 'ent_iob_',\n",
              " 'ent_kb_id',\n",
              " 'ent_kb_id_',\n",
              " 'ent_type',\n",
              " 'ent_type_',\n",
              " 'get_extension',\n",
              " 'has_dep',\n",
              " 'has_extension',\n",
              " 'has_head',\n",
              " 'has_morph',\n",
              " 'has_vector',\n",
              " 'head',\n",
              " 'i',\n",
              " 'idx',\n",
              " 'iob_strings',\n",
              " 'is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'left_edge',\n",
              " 'lefts',\n",
              " 'lemma',\n",
              " 'lemma_',\n",
              " 'lex',\n",
              " 'lex_id',\n",
              " 'like_email',\n",
              " 'like_num',\n",
              " 'like_url',\n",
              " 'lower',\n",
              " 'lower_',\n",
              " 'morph',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'nbor',\n",
              " 'norm',\n",
              " 'norm_',\n",
              " 'orth',\n",
              " 'orth_',\n",
              " 'pos',\n",
              " 'pos_',\n",
              " 'prefix',\n",
              " 'prefix_',\n",
              " 'prob',\n",
              " 'rank',\n",
              " 'remove_extension',\n",
              " 'right_edge',\n",
              " 'rights',\n",
              " 'sent',\n",
              " 'sent_start',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'set_morph',\n",
              " 'shape',\n",
              " 'shape_',\n",
              " 'similarity',\n",
              " 'subtree',\n",
              " 'suffix',\n",
              " 'suffix_',\n",
              " 'tag',\n",
              " 'tag_',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab',\n",
              " 'whitespace_']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "dir(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KiexNKgJuj9T",
        "outputId": "40c057ae-655f-41be-a1b4-b2a7f5744243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "type(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lINs5Lt9uj9U",
        "outputId": "afb76e67-3097-492c-d71e-51fb601cbea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "type(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0yCjSWVduj9V",
        "outputId": "2ef1f678-c5c0-4d58-b462-cf685cd724da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.token.Token"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "type(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "scrolled": true,
        "id": "M73NpNa6uj9V",
        "outputId": "da1ad7d4-5ae3-4d6f-c65e-b26487a4eea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke3fAhoKuj9W"
      },
      "source": [
        "<h3>Span object</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "VaHGf-jVuj9W",
        "outputId": "d463201f-33d9-4c4f-e8d6-d2a2540192a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dr. Strange loves pav bhaji"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "span = doc[0:5]\n",
        "span"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rSjd_RS_uj9X",
        "outputId": "3e5b94c2-81c8-4e9f-cef8-aa6f85e9745f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "type(span)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udOd6_TUuj9Y"
      },
      "source": [
        "<h3>Token attributes</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uP4GjLfGuj9Y"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"Tony gave two $ to Peter.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ToPjyRhWuj9Y",
        "outputId": "dfc6022f-3ab6-4767-f122-cefd52dc6d84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tony"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "token0 = doc[0]\n",
        "token0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SNN8PzzKuj9Z",
        "outputId": "1110b935-4db7-4c3a-a160-8d42b28a4c7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# alpha - alphabetic\n",
        "token0.is_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "scrolled": true,
        "id": "HlODEhvUuj9Z",
        "outputId": "f5d09f83-851c-4385-b4b8-8e182c198aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "token0.like_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "UleJMa1guj9Z",
        "outputId": "bc20ec90-0453-46e6-d5b8-c2f57bd4e8ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "two"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "token2 = doc[2]\n",
        "token2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token2.text"
      ],
      "metadata": {
        "id": "tPnBrNRLGP5o",
        "outputId": "24c8bae6-7a2e-4f97-a0b8-977931c87b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'two'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "L9FgD0Qauj9a",
        "outputId": "6ad34afc-2d9c-4902-dd0c-de7c51e642d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "token2.like_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "CGGz8ZRiuj9a"
      },
      "outputs": [],
      "source": [
        "token3 = doc[3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token3"
      ],
      "metadata": {
        "id": "bPMw7YdcGjw2",
        "outputId": "da8dd726-4edb-4706-9fe0-ce9a18111e6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "$"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(token3)"
      ],
      "metadata": {
        "id": "-Q8bwfj1GoqX",
        "outputId": "01f92a2e-f583-4d67-c65c-d7ae7910afc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.token.Token"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token3.text"
      ],
      "metadata": {
        "id": "wHKqIVfyGlks",
        "outputId": "52701e49-a967-4ed1-8c76-b4f37d41c9d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(token3.text)"
      ],
      "metadata": {
        "id": "EfroayaZGqls",
        "outputId": "1f477b0b-0e21-41f2-9ef8-5ddf4c217a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "iYmwLmNvuj9a",
        "outputId": "fa37c1d3-e5b5-40ba-c7ca-5da8b8bd84f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "token3.like_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "nGxw1EBruj9a",
        "outputId": "6d970cde-b83f-414c-bbc7-ed9d1bd172dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "token3.is_currency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "scrolled": true,
        "id": "n-B_JT71uj9b",
        "outputId": "acb3059b-9f09-40d2-a149-5fbf123d5989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tony ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
            "gave ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
            "two ==> index:  2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
            "$ ==> index:  3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
            "to ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
            "Peter ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
            ". ==> index:  6 is_alpha: False is_punct: True like_num: False is_currency: False\n"
          ]
        }
      ],
      "source": [
        "for token in doc:\n",
        "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha,\n",
        "          \"is_punct:\", token.is_punct,\n",
        "          \"like_num:\", token.like_num,\n",
        "          \"is_currency:\", token.is_currency,\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xWkAHsuj9b"
      },
      "source": [
        "<h3>Collecting email ids of students from students information sheet</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "rpcWkxQDuj9c",
        "outputId": "204ffbce-55eb-457d-b58f-9862ab07544e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dayton high school, 8th grade students information\\n',\n",
              " '==================================================\\n',\n",
              " '\\n',\n",
              " 'Name\\tbirth day   \\temail\\n',\n",
              " '-----\\t------------\\t------\\n',\n",
              " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
              " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
              " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
              " 'Joe      1 May, 1997    joe@root.com']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "with open(\"students.txt\") as f:\n",
        "    text = f.readlines()\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ZdZgNir9uj9c",
        "outputId": "9489e948-e139-4ea8-ab2d-1d100a258bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "text = \" \".join(text)\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "vwIXtMczuj9c",
        "outputId": "78cc9160-b812-46d7-b591-42edc9d38b96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['virat@kohli.com',\n",
              " 'maria@sharapova.com',\n",
              " 'serena@williams.com',\n",
              " 'joe@root.com']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "doc = nlp(text)\n",
        "emails = []\n",
        "for token in doc:\n",
        "    if token.like_email:\n",
        "        emails.append(token.text)\n",
        "emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96r2zUrmuj9d"
      },
      "source": [
        "<h3>Support in other languages</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt2jp1j5uj9d"
      },
      "source": [
        "Spacy support many language models. Some of them do not support pipelines though!\n",
        "https://spacy.io/usage/models#languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "scrolled": false,
        "id": "5Luanu6cuj9d",
        "outputId": "aa8e96da-bd41-4ad7-d8d2-581e244a1eb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "भैया False\n",
            "जी False\n",
            "! False\n",
            "5000 False\n",
            "₹ True\n",
            "उधार False\n",
            "थे False\n",
            "वो False\n",
            "वापस False\n",
            "देदो False\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.blank(\"hi\")\n",
        "doc = nlp(\"भैया जी! 5000 ₹ उधार थे वो वापस देदो\")\n",
        "for token in doc:\n",
        "    print(token, token.is_currency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukLMYwf5uj9e"
      },
      "source": [
        "<h3>Customizing tokenizer</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "orhsydPsuj9f",
        "outputId": "fcd89688-998f-4b7c-b205-f0be04a2b4b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "from spacy.symbols import ORTH\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "tokens = [token.text for token in doc]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "scrolled": true,
        "id": "502Y95ZQuj9f",
        "outputId": "0a03ebfe-c40b-4aae-bf4b-c410bc662c34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['give ',\n",
              " 'me ',\n",
              " 'me',\n",
              " 'double',\n",
              " 'cheese',\n",
              " 'extra',\n",
              " 'large',\n",
              " 'healthy',\n",
              " 'pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "nlp.tokenizer.add_special_case(\"gimme\", [\n",
        "    {ORTH: \"gim\"},\n",
        "    {ORTH: \"me\"},\n",
        "])\n",
        "# burda dusune bilersen ki, birbasa gimme sozunu give ve me olaraq yazsin. bu olmur. cunki hardan boleceyini bilmir. bunu sonradan etmek olur.\n",
        "# asagida onu da etmeyin usulu var -> amma en axirinci yorumda olan kodu run edenden sonra bura isleyir. Pipeline movzusudur\n",
        "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "tokens = [token.text for token in doc]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "id": "mixWETsyPGJq",
        "outputId": "2e096dfb-2d4f-4fd9-f73e-494a9dbcc27e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('replace_tokens', <function __main__.replace_tokens(doc)>)]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bu altdaki kimi bir seyi ferqli seye deyismek umumiyyetle mumkun deyil bu usulla. Pipeline will come to stage.\n",
        "# nlp.tokenizer.add_special_case(\"gim\", [\n",
        "#     {ORTH: \"give\"}\n",
        "# ])\n",
        "\n",
        "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "tokens = [token.text for token in doc]\n",
        "tokens"
      ],
      "metadata": {
        "id": "7tPhIb_UNXLb",
        "outputId": "1cf13e98-e659-4236-ca50-e817a146b0b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E997] Tokenizer special cases are not allowed to modify the text. This would map 'gim' to 'give' given token attributes '[{65: 'give'}]'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-70d0fdc310d3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m nlp.tokenizer.add_special_case(\"gim\", [\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"give\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m ])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gimme double cheese extra large healthy pizza\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E997] Tokenizer special cases are not allowed to modify the text. This would map 'gim' to 'give' given token attributes '[{65: 'give'}]'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# from spacy.tokenizer import Tokenizer\n",
        "# from spacy.tokens import Doc\n",
        "\n",
        "# nlp = spacy.blank(\"en\")\n",
        "\n",
        "# # Define a custom special case for \"gimme\"\n",
        "# nlp.tokenizer.add_special_case(\"gimme\", [\n",
        "#     {ORTH: \"gim\"},\n",
        "#     {ORTH: \"me\"},\n",
        "# ])\n",
        "\n",
        "# # Define a custom special case for converting \"gim\" to \"give\"\n",
        "# nlp.tokenizer.add_special_case(\"gim\", [\n",
        "#     {ORTH: \"give\"},\n",
        "# ])\n",
        "\n",
        "# # Create a Doc object\n",
        "# doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "\n",
        "# # Process the Doc with the modified tokenizer\n",
        "# tokens = [token.text for token in doc]\n",
        "# tokens\n"
      ],
      "metadata": {
        "id": "hDAeEBBPN1CA"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# from spacy.tokens import Doc\n",
        "\n",
        "# # Define a custom pipeline component to replace tokens\n",
        "# def replace_tokens(doc):\n",
        "#     new_tokens = []\n",
        "#     for token in doc:\n",
        "#         # Replace 'gim' with 'give'\n",
        "#         if token.text == 'gim':\n",
        "#             new_tokens.extend([Doc(token.doc.vocab, words=['give']), Doc(token.doc.vocab, words=['me'])])\n",
        "#         else:\n",
        "#             new_tokens.append(token)\n",
        "#     return Doc(doc.vocab, words=[t.text for t in new_tokens])\n",
        "\n",
        "# # Load spaCy model\n",
        "# nlp = spacy.blank(\"en\")\n",
        "\n",
        "# # Add the custom pipeline component\n",
        "# nlp.add_pipe(replace_tokens, name=\"replace_tokens\", last=True)\n",
        "\n",
        "# # Process the text\n",
        "# doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "\n",
        "# # Get the final tokens\n",
        "# tokens = [token.text for token in doc]\n",
        "# print(tokens)\n"
      ],
      "metadata": {
        "id": "dg6i7Bp0OFh5"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# from spacy.tokens import Doc\n",
        "# from spacy.language import Language\n",
        "\n",
        "# # Define a custom pipeline component to replace tokens\n",
        "# @Language.component(\"replace_tokens\")\n",
        "# def replace_tokens(doc):\n",
        "#     new_tokens = []\n",
        "#     for token in doc:\n",
        "#         # Replace 'gim' with 'give'\n",
        "#         if token.text == 'gim':\n",
        "#             new_tokens.extend([Doc(token.doc.vocab, words=['give']), Doc(token.doc.vocab, words=['me'])])\n",
        "#         else:\n",
        "#             new_tokens.append(token)\n",
        "#     return Doc(doc.vocab, words=[t.text for t in new_tokens])\n",
        "\n",
        "# # Load spaCy model\n",
        "# nlp = spacy.blank(\"en\")\n",
        "\n",
        "# # Add the custom pipeline component\n",
        "# nlp.add_pipe(\"replace_tokens\", last=True)\n",
        "\n",
        "# # Process the text\n",
        "# doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "\n",
        "# # Get the final tokens\n",
        "# tokens = [token.text for token in doc]\n",
        "# print(tokens)\n"
      ],
      "metadata": {
        "id": "PtLswRZhOM8z"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCuJ2KFLuj9g"
      },
      "source": [
        "<h3>Sentence Tokenization or Segmentation</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Pq21EexPuj9i",
        "outputId": "03cc7f0e-87cf-4ca3-c841-6b945fd24495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-220059dad4ce>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UovdGPweuj9j",
        "outputId": "fff0594c-daa1-40be-ddaa-4505e7aa048d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjPs7A7iuj9j",
        "outputId": "d8effe40-443d-46bb-c788-7f431fd37c6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x1caadca0680>"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.add_pipe('sentencizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9hdQSA0uj9k",
        "outputId": "a761d8b3-a7fd-4193-ffdc-22d431d6749c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dr. Strange loves pav bhaji of mumbai.\n",
            "Hulk loves chat of delhi\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
        "for sentence in doc.sents:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDsc86E_uj9k",
        "outputId": "8674c2dd-d88d-4f2d-9e82-e985fd183ed2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGvmmySiuj9l"
      },
      "source": [
        "<h3>Exercise</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR1YwsPuuj9l"
      },
      "source": [
        "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
        "\n",
        "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JQ-6nr_uj9m"
      },
      "outputs": [],
      "source": [
        "text='''\n",
        "Look for data to help you address the question. Governments are good\n",
        "sources because data from public research is often freely available. Good\n",
        "places to start include http://www.data.gov/, and http://www.science.\n",
        "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/,\n",
        "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "'''\n",
        "\n",
        "# TODO: Write code here\n",
        "# Hint: token has an attribute that can be used to detect a url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-H8z-9huj9m"
      },
      "source": [
        "(2) Extract all money transaction from below sentence along with currency. Output should be,\n",
        "\n",
        "two $\n",
        "\n",
        "500 €"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYHVmXfvuj9n"
      },
      "outputs": [],
      "source": [
        "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
        "\n",
        "# TODO: Write code here\n",
        "# Hint: Use token.i for the index of a token and token.is_currency for currency symbol detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mNxSs7Tuj9n"
      },
      "source": [
        "[Click me to see a solution](https://github.com/codebasics/nlp-tutorials/blob/main/4_tokenization/spacy_tokenizer_exercise_solution.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ6spy47uj9n"
      },
      "source": [
        "<h3>Further Reading</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChXmprO-uj9n"
      },
      "source": [
        "https://spacy.io/usage/linguistic-features#tokenization"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}